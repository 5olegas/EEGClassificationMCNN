{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a refactored version of a Multiscale Convolutional Network solution for NeuroHack at Yandex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.signal import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = \"hackaton_data/train.h5\"\n",
    "test_path = \"hackaton_data/test.h5\"\n",
    "model_dump_path = \"hackaton_data/convnet-multiscale-true-01988\"\n",
    "\n",
    "slice_len = 1125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read train data into a dict of (subject_id, (X, y)) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subjects = {}\n",
    "with h5py.File(train_path, \"r\") as data_file:\n",
    "    for subject, subject_data in data_file.items():\n",
    "        X = subject_data[\"data\"][:]\n",
    "        y = subject_data[\"labels\"][:][0]\n",
    "        subjects[subject] = (X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split our data into train and local validation sets. For local validation we select random slices of slice_len. They may overlap with slices in train set, this split is not optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_val_split(X, y):\n",
    "    start_indices = list(range(0, len(X) - slice_len))\n",
    "    y = y[:len(start_indices)]\n",
    "    indices_train, indices_test, _, _ = train_test_split(start_indices, y)\n",
    "    return {\"train_ind\": indices_train, \"train_y\": y_train, \"X\": X, \"y\": y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    X, y = subjects[subject][0], subjects[subject][1]\n",
    "    X = X.T\n",
    "    subjects[subject] = train_val_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define utility function to convert class labels to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_onehot(y):\n",
    "    onehot = np.zeros(3)\n",
    "    onehot[y] = 1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a funtion that will select random subject and find a random subsequence of consistent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_slice(slice_len, val=False):\n",
    "    subject_data = random.choice(list(subjects.values()))\n",
    "    if val is True:\n",
    "        indices, y, X = subject_data[\"val_ind\"], subject_data[\"y\"], subject_data[\"X\"]\n",
    "    else:\n",
    "        indices, y, X = subject_data[\"train_ind\"], subject_data[\"y\"], subject_data[\"X\"]\n",
    "    \n",
    "    while True:\n",
    "        slice_start = random.choice(indices)\n",
    "        slice_end = slice_start + slice_len\n",
    "        slice_x = X[slice_start:slice_end]\n",
    "        slice_y = y[slice_start:slice_end]\n",
    "        \n",
    "        if len(set(slice_y)) == 1:\n",
    "            return slice_x, to_onehot(slice_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a generator that will yield batches of resampled input time series and according class labels in infinite loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(batch_size, slice_len, val=False):\n",
    "    while True:\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "        \n",
    "        for i in range(0, batch_size):\n",
    "            x, y = generate_slice(slice_len, val=val)\n",
    "            batch_x.append(x)\n",
    "            batch_y.append(y)\n",
    "            \n",
    "        y = np.array(batch_y)\n",
    "        \n",
    "        x_256 = np.array([resample(i, 256) for i in batch_x])\n",
    "        x_500 = np.array([resample(i, 500) for i in batch_x])\n",
    "        x = np.array([i for i in batch_x])\n",
    "        yield ([x_256, x_500, x], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build a neural network. Import all needed layers and keras utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Convolution1D, Dense, Dropout, Input, merge, GlobalMaxPooling1D\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function builds a base neural net model that performs feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_base_model(input_len, fsize):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    input_seq = Input(shape=(input_len, 24))\n",
    "    nb_filters = 150\n",
    "    convolved = Convolution1D(nb_filters, fsize, border_mode=\"same\", activation=\"tanh\")(input_seq)\n",
    "    processed = GlobalMaxPooling1D()(convolved)\n",
    "    compressed = Dense(150, activation=\"tanh\")(processed)\n",
    "    compressed = Dropout(0.3)(compressed)\n",
    "    compressed = Dense(150, activation=\"tanh\")(compressed)\n",
    "    model = Model(input=input_seq, output=compressed)            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and compile a graph with 3 inputs and one output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input256_seq = Input(shape=(256, 24))\n",
    "input500_seq = Input(shape=(500, 24))\n",
    "input1125_seq = Input(shape=(1125, 24))\n",
    "    \n",
    "base_network256 = get_base_model(256, 4)\n",
    "base_network500 = get_base_model(500, 7)\n",
    "base_network1125 = get_base_model(1125, 10)\n",
    "\n",
    "embedding_256 = base_network256(input256_seq)\n",
    "embedding_500 = base_network500(input500_seq)\n",
    "embedding_1125 = base_network1125(input1125_seq)\n",
    "    \n",
    "merged = merge([embedding_256, embedding_500, embedding_1125], mode=\"concat\")\n",
    "out = Dense(3, activation='softmax')(merged)\n",
    "    \n",
    "model = Model(input=[input256_seq, input500_seq, input1125_seq], output=out)\n",
    "    \n",
    "opt = RMSprop(lr=0.005, clipvalue=10**6)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will train the model from scratch, lets load it from the model dump instead (take a look at next code cell)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "nb_epoch = 100000\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
    "samples_per_epoch = 1000\n",
    "\n",
    "model.fit_generator(data_generator(batch_size=50, slice_len=slice_len), samples_per_epoch, nb_epoch, \n",
    "                    callbacks=[earlyStopping], verbose=1, nb_val_samples=2000,\n",
    "                    validation_data=data_generator(batch_size=50, slice_len=slice_len, val=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(\"hackaton_data/convnet-multiscale-true-01988\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read test data into a nested structure with multiple chunks for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with h5py.File(\"hackaton_data/test.h5\", \"r\") as data_file:\n",
    "    test = {}\n",
    "    for subject, subject_data in data_file.items():\n",
    "        test[subject] = {}\n",
    "        for chunk_id, chunk in data_file[subject].items():\n",
    "            test[subject][chunk_id] = chunk[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test['subject_0']['chunk_0'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# utility function that performs resampling of input timeseries \n",
    "def multiscale(chunk):\n",
    "    resampled_256 = resample(chunk, 256)\n",
    "    resampled_500 = resample(chunk, 500)\n",
    "    return [resampled_256, resampled_500, chunk]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make prediction for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for subj in test:\n",
    "    for chunk in tqdm(test[subj]):\n",
    "        data = {}\n",
    "        data[\"subject_id\"] = int(subj.split(\"_\")[-1])\n",
    "        data[\"chunk_id\"] = int(chunk.split(\"_\")[-1])\n",
    "        arr = test[subj][chunk].T\n",
    "        preds = model.predict([np.array([i]) for i in multiscale(arr)])[0]\n",
    "        data[\"class_0_score\"] = preds[0]\n",
    "        data[\"class_1_score\"] = preds[1]\n",
    "        data[\"class_2_score\"] = preds[2]\n",
    "        for i in range(0, 1125):\n",
    "            data[\"tick\"] = i\n",
    "            df.append(data.copy())\n",
    "df = pd.DataFrame(df)\n",
    "df = df[[\"subject_id\", \"chunk_id\", \"tick\", \"class_0_score\",\n",
    "         \"class_1_score\",\"class_2_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save submission to .csv\n",
    "df.to_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
